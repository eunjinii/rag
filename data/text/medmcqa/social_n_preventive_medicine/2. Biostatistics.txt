Descriptive Statistics provides methods for summarizing and presenting data in a meaningful way. Measures of central tendency describe the typical or central value of a dataset. The mean is the arithmetic average, calculated by summing all values and dividing by the number of values; it is sensitive to extreme values (outliers). The median is the middle value when the data are arranged in order; it is less affected by outliers. The mode is the most frequent value in a dataset. Measures of dispersion describe the spread or variability of the data. The standard deviation (SD) measures the average distance of data points from the mean; a larger SD indicates greater variability. The variance is the square of the standard deviation. The range is the difference between the maximum and minimum values. The interquartile range (IQR) is the difference between the 75th percentile (third quartile) and the 25th percentile (first quartile), representing the middle 50% of the data; it is less sensitive to outliers than the range. Data visualization techniques are used to present data graphically. Histograms display the frequency distribution of continuous data, showing the shape of the distribution. Box plots display the median, quartiles, and any outliers in a dataset. Scatter plots show the relationship between two continuous variables. Probability distributions describe the probabilities of different outcomes for a random variable. The normal distribution (Gaussian distribution) is a bell-shaped, symmetrical distribution that is commonly used in statistics; many biological variables approximate a normal distribution. The standard normal distribution is a normal distribution with a mean of 0 and a standard deviation of 1. The binomial distribution describes the probability of a certain number of successes in a fixed number of independent trials, where each trial has only two possible outcomes (success or failure). The Poisson distribution describes the probability of a certain number of events occurring within a fixed interval of time or space, when events occur independently and at a constant rate.

Inferential Statistics uses sample data to make inferences about populations. Hypothesis testing is a formal procedure for evaluating a claim about a population parameter. The null hypothesis (H0) is a statement of no effect or no difference. The alternative hypothesis (Ha) is a statement that contradicts the null hypothesis. A statistical test is used to assess the evidence against the null hypothesis. The p-value is the probability of observing data as extreme as, or more extreme than, the observed data, assuming the null hypothesis is true. A small p-value (typically less than a significance level, Î±, often 0.05) provides evidence against the null hypothesis, leading to its rejection. A type I error occurs when the null hypothesis is rejected when it is actually true (false positive). A type II error occurs when the null hypothesis is not rejected when it is actually false (false negative). Confidence intervals provide a range of values within which the true population parameter is likely to lie, with a specified level of confidence (e.g., 95% confidence interval). The width of the confidence interval reflects the precision of the estimate; a narrower interval indicates greater precision. Statistical tests are used to compare groups or assess relationships between variables. The t-test is used to compare the means of two groups. Independent samples t-tests are used for comparing means of two independent groups, while paired samples t-tests are used for comparing means of two related groups (e.g., before and after measurements on the same individuals). The chi-square test is used to assess the association between two categorical variables. It compares the observed frequencies in a contingency table to the expected frequencies under the assumption of no association. ANOVA (analysis of variance) is used to compare the means of three or more groups. It tests whether there is a statistically significant difference between the group means. Correlation measures the strength and direction of the linear relationship between two continuous variables. The Pearson correlation coefficient (r) ranges from -1 to +1, with -1 indicating a perfect negative correlation, +1 indicating a perfect positive correlation, and 0 indicating no linear correlation. Regression analysis is used to model the relationship between a dependent variable and one or more independent variables. Linear regression models the relationship between a continuous dependent variable and one or more continuous or categorical independent variables, assuming a linear relationship. Logistic regression models the relationship between a binary dependent variable (e.g., presence or absence of disease) and one or more independent variables. Multiple regression allows for the inclusion of multiple independent variables in the model, controlling for the effects of other variables. Survival analysis is used to analyze time-to-event data, such as time to death or time to disease recurrence. Kaplan-Meier curves are used to estimate the survival function, which represents the probability of surviving beyond a certain time. The log-rank test is used to compare survival curves between two or more groups. Cox proportional hazards regression models the relationship between survival time and one or more independent variables, allowing for the estimation of hazard ratios, which represent the relative risk of an event occurring in one group compared to another. Power analysis is used to determine the sample size needed to detect a statistically significant effect with a specified level of power (the probability of rejecting the null hypothesis when it is false). Sample size calculations are important for ensuring that studies have adequate power to detect meaningful effects. Statistical software packages, such as R, SPSS, SAS, and Stata, are commonly used to perform statistical analyses. These packages provide a wide range of statistical procedures and tools for data management, analysis, and visualization.