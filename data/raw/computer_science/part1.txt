# Part1: Algorithms and Data Structures

Chapter 1: Analysis of Algorithms: Beyond Asymptotic Dominance

- 1.1 Review of Asymptotic Notation: Big-O, Big-Omega, Big-Theta, little-o, little-omega. Formal definitions and limit-based interpretations. Emphasis on *precise* mathematical formulations.
- 1.2 Amortized Analysis:
    - 1.2.1 Motivation: Limitations of worst-case analysis for dynamic data structures.
    - 1.2.2 Techniques: Aggregate analysis, accounting method (banker's method), potential method (physicist's method). Rigorous proofs using potential functions.
    - 1.2.3 Examples: Dynamic arrays (vector resizing), splay trees, Fibonacci heaps (detailed analysis). Comparison of different amortization techniques.
- 1.3 Probabilistic Analysis and Randomized Algorithms:
    - 1.3.1 Probability Review: Random variables, expectation, variance, linearity of expectation, Markov's inequality, Chebyshev's inequality, Chernoff bounds.
    - 1.3.2 Average-Case Analysis: Analyzing algorithms under specific input distributions. Challenges and limitations.
    - 1.3.3 Randomized Algorithms: Las Vegas vs. Monte Carlo algorithms. Examples: Randomized quicksort (detailed analysis of expected runtime and probability of success), randomized selection, randomized primality testing (Miller-Rabin).
    - 1.3.4 Derandomization: Techniques for converting randomized algorithms into deterministic ones. Method of conditional probabilities.
- 1.4 Competitive Analysis of Online Algorithms:
    - 1.4.1 Online vs. Offline Algorithms: Defining the online setting and its challenges.
    - 1.4.2 Competitive Ratio: Formal definition. Deterministic and randomized competitive ratios.
    - 1.4.3 Examples: Paging algorithms (LRU, FIFO, optimal offline algorithm), the k-server problem, list accessing problem. Lower bounds on competitive ratios.
- 1.5 Lower Bounds:
    - 1.5.1 Decision Tree Model: Proving lower bounds for comparison-based sorting.
    - 1.5.2 Adversary Arguments: Constructing adversarial inputs to force algorithms to perform poorly. Examples: Finding the median, searching in a sorted array.
    - 1.5.3 Information-Theoretic Lower Bounds: Using information theory to establish limits on computational efficiency.
    - 1.5.4 Communication Complexity: Lower bounds for distributed and parallel algorithms.
- 1.6 Beyond Worst-Case Analysis:
    - 1.6.1 Smooth Analysis: Analyzing algorithms under slight perturbations of the input. Applications to linear programming (simplex method).
    - 1.6.2 Parameterized Complexity: Analyzing algorithms in terms of input size *and* other parameters. Fixed-parameter tractability (FPT). Kernelization. W-hierarchy.
    - 1.6.3 Instance Optimality: Demonstrating that an algorithm performs as well as possible, not just in the worst-case, but on *every* instance, up to a constant factor.

Chapter 2: Advanced Data Structures

- 2.1 Self-Adjusting Data Structures:
    - 2.1.1 Splay Trees: Splaying operation, amortized analysis of splay trees (potential function proof), working set property, static optimality, dynamic optimality conjecture.
    - 2.1.2 Link-Cut Trees: Representing dynamic trees, path decomposition, amortized analysis, applications to network flow.
    - 2.1.3 Tango Trees: A data structure that achieves *O*(log log *n*) competitive ratio for the dynamic optimality problem.
- 2.2 Persistent Data Structures:
    - 2.2.1 Motivation: Maintaining multiple versions of a data structure. Applications in version control, computational geometry, and functional programming.
    - 2.2.2 Techniques: Path copying, fat nodes, node splitting. Trade-offs between space and time complexity.
    - 2.2.3 Examples: Persistent arrays, persistent linked lists, persistent search trees.
- 2.3 Concurrent Data Structures:
    - 2.3.1 Challenges: Data races, deadlocks, livelocks, starvation. Memory consistency models.
    - 2.3.2 Techniques: Locks (fine-grained, coarse-grained), lock-free data structures (using atomic operations like compare-and-swap), transactional memory.
    - 2.3.3 Examples: Concurrent queues, concurrent hash tables, concurrent skip lists. Linearizability.
- 2.4 Succinct Data Structures:
    - 2.4.1 Motivation: Representing data using space close to the information-theoretic lower bound.
    - 2.4.2 Techniques: Rank and select operations, bit vectors, wavelet trees, compressed suffix arrays.
    - 2.4.3 Examples: Representing trees, graphs, and text using succinct data structures.
- 2.5 Geometric Data Structures:
    - 2.5.1 Range Searching: k-d trees, range trees, quadtrees. Orthogonal range searching, analysis of query time and space complexity.
    - 2.5.2 Point Location: Planar subdivisions, trapezoidal maps.
    - 2.5.3 Nearest Neighbor Search: Voronoi diagrams, Delaunay triangulations.
    - 2.5.4 Interval Trees and Segment Trees: Managing intervals and performing queries on overlapping intervals.
- 2.6 External Memory Data Structures:
    - 2.6.1 The External Memory Model: Accounting for I/O costs between disk and main memory.
    - 2.6.2 B-Trees: Structure, search, insertion, deletion, analysis in the external memory model. Variants (B+ trees).
    - 2.6.3 Cache-Oblivious Algorithms: Designing algorithms that perform well in the external memory model without knowing the block size or memory size. Examples: cache-oblivious sorting, cache-oblivious search trees.

Chapter 3: Advanced Algorithm Design Techniques

- 3.1 Dynamic Programming (Advanced):
    - 3.1.1 Optimal Substructure and Overlapping Subproblems: Formalizing these concepts.
    - 3.1.2 Techniques: Memoization, tabulation, top-down vs. bottom-up approaches.
    - 3.1.3 Examples: Sequence alignment (Needleman-Wunsch, Smith-Waterman with affine gap penalties), optimal binary search trees, matrix chain multiplication, longest common subsequence.
    - 3.1.4 Divide-and-Conquer DP: Combining dynamic programming with divide and conquer for improved efficiency. Examples: closest pair of points.
    - 3.1.5 DP Optimizations: Techniques like the Knuth-Yao speedup for specific DP recurrences.
- 3.2 Linear Programming and its Applications:
    - 3.2.1 Linear Programming Fundamentals: Formulating problems as linear programs, standard form, feasible regions, objective functions, duality.
    - 3.2.2 Simplex Algorithm: Detailed explanation, geometric interpretation, pivoting rules, handling degeneracy, theoretical analysis (worst-case exponential time, smoothed analysis).
    - 3.2.3 Interior-Point Methods: Karmarkar's algorithm, polynomial-time algorithms for linear programming.
    - 3.2.4 Applications: Network flow, matching, scheduling, resource allocation, game theory.
    - 3.2.5 Integer Programming: Branch and bound, cutting plane methods.
- 3.3 Approximation Algorithms (Advanced):
    - 3.3.1 NP-Hardness and Inapproximability: Review of NP-completeness. Proofs of inapproximability (using gap reductions).
    - 3.3.2 Techniques: Greedy algorithms, local search, primal-dual methods, LP rounding, semidefinite programming (SDP) relaxation.
    - 3.3.3 Examples: Vertex cover, set cover, traveling salesperson problem (TSP), knapsack problem, maximum satisfiability (MAX-SAT). Analysis of approximation ratios and performance guarantees.
    - 3.3.4 PTAS and FPTAS: Polynomial-time approximation schemes and fully polynomial-time approximation schemes.
- 3.4 Network Flow Algorithms (Advanced):
    - 3.4.1 Ford-Fulkerson Algorithm: Augmenting paths, residual networks, max-flow min-cut theorem.
    - 3.4.2 Edmonds-Karp Algorithm: Shortest augmenting paths, polynomial-time complexity.
    - 3.4.3 Push-Relabel Algorithms: Preflow-push algorithms, analysis of running time.
    - 3.4.4 Minimum Cost Flow: Cycle canceling algorithms, successive shortest path algorithms.
    - 3.4.5 Applications: Bipartite matching, assignment problems, image segmentation.
- 3.5 String Algorithms (Advanced):
    - 3.5.1 Suffix Trees and Suffix Arrays: Construction algorithms (Ukkonen's algorithm, linear-time construction of suffix arrays), applications (string matching, longest common substring, repeat finding).
    - 3.5.2 Burrows-Wheeler Transform (BWT): Relationship to suffix arrays, applications in data compression.
    - 3.5.3 Edit Distance (Advanced): Levenshtein distance, algorithms for computing edit distance with different cost models.
    - 3.5.4 Approximate String Matching: Algorithms for finding occurrences of a pattern in a text with a limited number of errors (e.g., using dynamic programming, bit-parallelism).
- 3.6 Computational Geometry Algorithms:
    - 3.6.1 Convex Hull: Algorithms for computing the convex hull of a set of points in 2D and 3D (Graham scan, Jarvis march, divide-and-conquer).
    - 3.6.2 Line Segment Intersection: Sweep line algorithm (Bentley-Ottmann algorithm), analysis of running time and space complexity.
    - 3.6.3 Voronoi Diagrams and Delaunay Triangulations: Algorithms for constructing Voronoi diagrams and Delaunay triangulations, applications in various fields.
    - 3.6.4 Geometric Optimization: Applying techniques like linear programming and parametric search to solve geometric optimization problems.