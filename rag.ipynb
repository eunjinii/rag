{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from functools import lru_cache\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Retriever with Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRetriever:\n",
    "    def __init__(\n",
    "            self,\n",
    "            raw_data_path:str=\"\",\n",
    "            retriever_type:str=\"contriever\",\n",
    "            chunk_size:int=256,\n",
    "            device:str=\"cuda\"\n",
    "        ):\n",
    "\n",
    "        self.raw_data_path = raw_data_path\n",
    "        self.retriever_type = retriever_type\n",
    "        self.chunk_size = chunk_size\n",
    "        self.device = device\n",
    "\n",
    "        self.texts = self._load_dataset()\n",
    "    \n",
    "        if self.retriever_type == \"medcpt\":\n",
    "            model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "        elif self.retriever_type == 'specter':\n",
    "            model_name = \"allenai/specter\"\n",
    "        else:\n",
    "            model_name = \"facebook/contriever\"\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=chunk_size)\n",
    "        # print(f\"Model and Tokenizer {model_name} loaded.\")\n",
    "        \n",
    "        # Dense - Contriever, MedCPT, Specter, RRF-2\n",
    "        if self.retriever_type in [\"contriever\", \"medcpt\", \"specter\", \"rrf2\"]:\n",
    "            text_embedding = self._compute_embeddings(self.texts)\n",
    "            text_embedding = text_embedding.cpu()\n",
    "            self.faiss_index = self._create_dense_retriever(text_embedding)\n",
    "            # print(\"FAISS index created.\")\n",
    "\n",
    "        # Sparse - BM25, TF-IDF\n",
    "        self.bm25_index = self._get_bm25_index(self.texts)\n",
    "        self.tfidf_retriever = TFIDFRetriever.from_texts(self.texts)\n",
    "\n",
    "    def _compute_embeddings(self, texts):\n",
    "        def mean_pooling(token_embeddings, mask):\n",
    "            token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "            sentence_embeddings = (token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]).detach()\n",
    "            return sentence_embeddings\n",
    "        \n",
    "        all_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 32):\n",
    "                batch_texts = texts[i:i+32]\n",
    "                inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = mean_pooling(outputs[0], inputs['attention_mask']).cpu()\n",
    "                embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "                all_embeddings.append(embeddings)\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    def _compute_query_embedding(self, query):\n",
    "        return self._compute_embeddings([query]).numpy()\n",
    "    \n",
    "    def _create_dense_retriever(self, text_embedding):\n",
    "        d = text_embedding.shape[1]\n",
    "        faiss_index = faiss.IndexFlatIP(d) # [IndexFlatIP, IndexFlatL2]\n",
    "        faiss_index.add(np.ascontiguousarray(text_embedding.numpy()))\n",
    "        return faiss_index\n",
    "    \n",
    "    def _get_dense_results(self, query, k=5):\n",
    "        query_embedding = self._compute_query_embedding(query)\n",
    "        distances, indices = self.faiss_index.search(query_embedding, k)\n",
    "        faiss_results = []\n",
    "        for idx_list, dist_list in zip(indices, distances):\n",
    "            for idx, dist in zip(idx_list, dist_list):\n",
    "                faiss_results.append((self.texts[idx], dist))\n",
    "        return faiss_results[:k]\n",
    "    \n",
    "    def _get_bm25_index(self, texts):\n",
    "        tokenized_corpus = [text.lower().split() for text in texts]\n",
    "        bm25_index = BM25Okapi(tokenized_corpus)\n",
    "        return bm25_index\n",
    "    \n",
    "    def _get_bm25_results(self, query):\n",
    "        query_tokens = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(query_tokens)\n",
    "        bm25_results = [(self.texts[i], bm25_scores[i]) for i in range(len(bm25_scores))]\n",
    "        bm25_results = sorted(bm25_results, key=lambda x: x[1], reverse=True)\n",
    "        return bm25_results\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        texts = []\n",
    "        for file_name in os.listdir(self.raw_data_path):\n",
    "            file_path = os.path.join(self.raw_data_path, file_name)\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    texts.append(f.read().strip())\n",
    "            elif file_name.endswith(\".csv\"):\n",
    "                df = pd.read_csv(file_path)\n",
    "                if \"summary\" in df.columns:\n",
    "                    texts.extend(df[\"summary\"].dropna().tolist())\n",
    "\n",
    "        concatenated_text = \" \".join(texts)\n",
    "        texts = self._split_text(concatenated_text, self.chunk_size)\n",
    "        # print(f\"Loaded {len(texts)} documents from directory: {self.raw_data_path}\")\n",
    "        return texts\n",
    "\n",
    "    def _ensemble_scores(self, *results_with_weights):\n",
    "        \"\"\"\n",
    "        Args: results_with_weights: Tuples of (results, weight) where results are lists of (text, score).\n",
    "        Returns: List of (text, combined_score) tuples sorted by score in descending order.\n",
    "        \"\"\"\n",
    "        combined_scores = defaultdict(float)\n",
    "\n",
    "        for results, weight in results_with_weights:\n",
    "            scores_dict = {text: score for text, score in results}\n",
    "            values = np.array(list(scores_dict.values()))\n",
    "            if len(values) > 0:\n",
    "                min_val, max_val = values.min(), values.max()\n",
    "                if max_val > min_val:\n",
    "                    scores_dict = {k: (v - min_val) / (max_val - min_val) for k, v in scores_dict.items()}\n",
    "\n",
    "            for text, score in scores_dict.items():\n",
    "                combined_scores[text] += weight * score\n",
    "\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results\n",
    "    \n",
    "    def _split_text(self, texts, chunk_size, chunk_overlap=50):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        return text_splitter.split_text(texts)\n",
    "\n",
    "    def search(self, query:str, k:int=5):\n",
    "        if self.retriever_type in [\"contriever\", \"medcpt\", \"specter\"]:\n",
    "            return self._get_dense_results(query, k=k)\n",
    "\n",
    "        elif self.retriever_type == \"bm25\":\n",
    "            return self._get_bm25_results(query)[:k]\n",
    "        \n",
    "        elif self.retriever_type == \"tfidf\":\n",
    "            return self.tfidf_retriever.invoke(query, k=k)\n",
    "        \n",
    "        elif self.retriever_type == \"rrf2\":\n",
    "            faiss_results = self._get_dense_results(query, k=k)\n",
    "            bm25_results = self._get_bm25_results(query)\n",
    "            return self._ensemble_scores((faiss_results, 0.5), (bm25_results, 0.5))[:k]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid retriever type. Choose from ['contriever', 'specter', 'medcpt', 'bm25', 'tfidf', 'rrf2']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Clinical neurology begins with the neurological examination, the most important diagnostic tool. It systematically assesses: Mental Status (level of consciousness, orientation, attention, memory, language, executive function); Cranial Nerves (I-XII);\n",
      "Score: 0.7006931900978088\n",
      "\n",
      "Text: of key concepts in Neurology. Remember to integrate your knowledge of basic neuroscience with clinical findings, and to practice the neurological examination until it becomes second nature. Continuous learning and staying current with new developments are\n",
      "Score: 0.6949046850204468\n",
      "\n",
      "Text: Diagnostic techniques in neurology include neuroimaging: computed tomography (CT: head CT, CT angiography) and magnetic resonance imaging (MRI: brain MRI, spinal cord MRI, MR angiography, MR spectroscopy). Positron emission tomography (PET) assesses brain\n",
      "Score: 0.692385733127594\n",
      "\n",
      "Text: Therapeutic approaches in neurology include pharmacological therapies: antiepileptic drugs (AEDs) for seizures, anti-Parkinsonian medications, disease-modifying therapies for MS, stroke medications (thrombolytics, antiplatelets, anticoagulants), and pain\n",
      "Score: 0.6690757870674133\n",
      "\n",
      "Text: Neurologic disorders encountered in General Medicine include stroke (ischemic and hemorrhagic; diagnosis, treatment, prevention), headache (migraine, tension, cluster), seizures and epilepsy (diagnosis, treatment), multiple sclerosis (MS: diagnosis,\n",
      "Score: 0.6470555067062378\n",
      "\n",
      "Text: are essential for diagnosis. Neuropharmacology focuses on how drugs affect the nervous system. Understanding neurotransmitters, receptors, and the blood-brain barrier is crucial for effective medication use. Commonly used neurologic medications include\n",
      "Score: 0.6420316100120544\n",
      "\n",
      "Text: it is a fundamental component of safe, effective, and ethical patient care. Neurology, the study of the nervous system, demands mastery of both fundamental neuroscience and clinical application. Neuroanatomy is paramount. The central nervous system (CNS)\n",
      "Score: 0.6371423006057739\n",
      "\n",
      "Text: use. Commonly used neurologic medications include antiepileptics, anti-Parkinsonian drugs, antidepressants, antipsychotics, and analgesics. Neurogenetics plays an increasing role. Inherited neurological disorders (Huntington's disease, muscular\n",
      "Score: 0.6331122517585754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retriever = MyRetriever(raw_data_path=\"../data/raw/medical_googleai\", retriever_type=\"contriever\")\n",
    "\n",
    "# Search query with ensemble of Specter + BM25\n",
    "query = \"What is neurology?\"\n",
    "results = retriever.search(query, k=8)\n",
    "\n",
    "# Print results\n",
    "for text, score in results:\n",
    "    print(f\"Text: {text}\\nScore: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2:1b\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (No RAG): You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\n",
      "Answer (No RAG): c\n"
     ]
    }
   ],
   "source": [
    "# Run the model with a prompt\n",
    "def run_simple_prompt(model, question, options):\n",
    "    template = '''You are an AI assistant for grad students.\n",
    "    Answer the question and select the most appropriate answer from the given choices.\n",
    "    You must return only the correct answer (a, b, c or d) and nothing else.\n",
    "    \n",
    "    Question: {question}\n",
    "    Choices:\n",
    "    {options}\n",
    "\n",
    "    You must return a single character (a, b, c or d). Do not provide any explanation.\n",
    "    '''\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    simple_chain = (prompt | model | StrOutputParser())\n",
    "    answer_without_rag = simple_chain.invoke({\"question\": question, \"options\": options})\n",
    "    return answer_without_rag\n",
    "\n",
    "question = \"You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\"\n",
    "options = [\"Buffer Analysis\", \"Network Analysis\", \"Raster Reclassification\", \"Thiessen Polygon Generation\"]\n",
    "answer_without_rag = run_simple_prompt(model, question, options)\n",
    "\n",
    "print(\"Query (No RAG):\", question)\n",
    "print(\"Answer (No RAG):\", answer_without_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model with a prompt and RAG\n",
    "def run_rag_chain(model, retriever, question, options, k=2):\n",
    "    def format_docs(docs):\n",
    "        if hasattr(docs[0], 'page_content'):\n",
    "            return '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            return '\\n\\n'.join([d[0] for d in docs])\n",
    "        # return '\\n\\n'.join([ d.page_content for d in docs])\n",
    "\n",
    "    template = '''\n",
    "    Here are the relevant summaries: {context}\n",
    "    \n",
    "    You are an AI assistant for graduate students.\n",
    "    Use the provided context to select the most accurate answer.\n",
    "    Return only one letter (a, b, c, or d) with no additional text or explanation.\n",
    "\n",
    "    Question: {question}\n",
    "    Potential choices: {options}\n",
    "\n",
    "    Output only one character (a, b, c, or d) with no explanation.\n",
    "    '''\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retrieved_docs = retriever.search(query=question, k=k)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    rag_chain = (prompt | model | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"context\": formatted_context, \"question\": question, \"options\": options}).strip()\n",
    "    return answer\n",
    "\n",
    "# question = \"You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\"\n",
    "# options = [\"Buffer Analysis\", \"Network Analysis\", \"Raster Reclassification\", \"Thiessen Polygon Generation\"]\n",
    "# answer = run_rag_chain(model, question, options)\n",
    "\n",
    "# print(\"Query (RAG):\", question)\n",
    "# print(\"Answer (RAG):\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMedMCQA dataset\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "\"\"\"\n",
    "Prompt-made Computer Science dataset\n",
    "\"\"\"\n",
    "# with open(\"../data/qna/computer_science.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "# qna_df = pd.DataFrame([{\n",
    "#     \"question\": item[\"question\"],\n",
    "#     \"options\": item[\"options\"],\n",
    "#     \"answer\": item[\"answer_idx\"]\n",
    "# } for item in data])\n",
    "\n",
    "\"\"\"\n",
    "GBaker/MedQA-USMLE-4-options-hf\n",
    "\"\"\"\n",
    "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options-hf\", split=\"test\") # 4 choices\n",
    "qna_df = dataset.to_pandas()\n",
    "qna_df.rename(columns={\"sent1\": \"question\"}, inplace=True)\n",
    "answer_mapping = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "qna_df['answer'] = qna_df['label'].map(answer_mapping)\n",
    "qna_df['options'] = qna_df.apply(lambda row: [row['ending0'], row['ending1'], row['ending2'], row['ending3']], axis=1)\n",
    "# print(qna_df.head())\n",
    "\n",
    "\"\"\"\n",
    "MedMCQA dataset\n",
    "\"\"\"\n",
    "# dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\") # 4 choices / subject_name\n",
    "# qna_df = dataset.to_pandas()\n",
    "# qna_df.rename(columns={\"opa\": \"optionA\", \"opb\": \"optionB\", \"opc\": \"optionC\", \"opd\": \"optionD\", \"cop\": \"answerindex\"}, inplace=True)\n",
    "# answer_mapping = {0: 'a', 1: 'b', 2: 'c', 3: 'd'}\n",
    "# qna_df['answer'] = qna_df['answerindex'].map(answer_mapping)\n",
    "# qna_df['options'] = qna_df.apply(lambda row: [row['optionA'], row['optionB'], row['optionC'], row['optionD']], axis=1)\n",
    "# # qna_df['subject_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NAIVEQ&A :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [01:32<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONTRIEVER :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [02:03<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPECTER :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [02:07<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEDCPT :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [02:00<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BM25 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [02:21<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TFIDF :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [01:51<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RRF2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1273/1273 [02:33<00:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Accuracy Result:\n",
      "| Retriever   | K / Chunk Size   |   Accuracy (%) |   Processing Time (s) |\n",
      "|:------------|:-----------------|---------------:|----------------------:|\n",
      "| NAIVEQ&A    | 2 / 256          |          27.49 |                 92.39 |\n",
      "| CONTRIEVER  | 2 / 256          |          22.15 |                123.23 |\n",
      "| SPECTER     | 2 / 256          |          23.8  |                127.03 |\n",
      "| MEDCPT      | 2 / 256          |          22.07 |                120.32 |\n",
      "| BM25        | 2 / 256          |          23.25 |                141.18 |\n",
      "| TFIDF       | 2 / 256          |          22.78 |                111.17 |\n",
      "| RRF2        | 2 / 256          |          21.76 |                153.42 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct_answers = 0\n",
    "total_samples = len(qna_df)\n",
    "\n",
    "chunk_size = 256\n",
    "k = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "for retriever_type in [\"naiveq&a\", \"contriever\", \"specter\", \"medcpt\", \"bm25\", \"tfidf\", \"rrf2\"]:\n",
    "    print(f\"\\n{retriever_type.upper()} :\")\n",
    "    \n",
    "    if retriever_type != \"naiveq&a\":\n",
    "        retriever = MyRetriever(\n",
    "            raw_data_path =\"../data/raw/computer_science\",\n",
    "            retriever_type=retriever_type,\n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for idx, (index, sample) in enumerate(tqdm(qna_df.iterrows(), total=len(qna_df))):\n",
    "        question = sample[\"question\"]\n",
    "        options = sample[\"options\"]\n",
    "        gt_answer = sample[\"answer\"].lower()\n",
    "\n",
    "        if retriever_type == \"naiveq&a\":\n",
    "            pred_answer = run_simple_prompt(model, question, options).lower()\n",
    "        else:\n",
    "            pred_answer = run_rag_chain(model, retriever, question, options, k).lower()\n",
    "\n",
    "        if pred_answer not in ['a', 'b', 'c', 'd']:\n",
    "            print(\"pred_answer:\", pred_answer)\n",
    "\n",
    "        correct_answers += int(pred_answer == gt_answer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    acc = correct_answers / total_samples * 100\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    results.append({\n",
    "        \"Retriever\": retriever_type.upper(),\n",
    "        \"K / Chunk Size\": f\"{k} / {chunk_size}\",\n",
    "        \"Accuracy (%)\": f\"{acc:.2f}\",\n",
    "        \"Processing Time (s)\": f\"{processing_time:.2f}\",\n",
    "    })\n",
    "\n",
    "    correct_answers = 0\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nðŸŽ¯ Accuracy Result:\")\n",
    "print(results_df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
