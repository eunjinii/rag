{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Retriever with Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRetriever:\n",
    "    def __init__(\n",
    "            self,\n",
    "            raw_data_path:str=\"\",\n",
    "            retriever_type:str=\"contriever\",\n",
    "            chunk_size:int=256,\n",
    "            device:str=\"cuda\"\n",
    "        ):\n",
    "\n",
    "        self.raw_data_path = raw_data_path\n",
    "        self.retriever_type = retriever_type\n",
    "        if retriever_type == 'domain_detector':\n",
    "            self.chunk_size = None\n",
    "        else:\n",
    "            self.chunk_size = chunk_size\n",
    "        self.device = device\n",
    "\n",
    "        self.texts = self._load_dataset()\n",
    "        \n",
    "        # Dense - Contriever, MedCPT, Specter, RRF-2\n",
    "        if self.retriever_type in [\"contriever\", \"specter\", \"longformer\", \"medcpt\", \"rrf2\"]:\n",
    "            if self.retriever_type == 'contriever':\n",
    "                model_name = \"facebook/contriever\"\n",
    "            elif self.retriever_type == 'specter':\n",
    "                model_name = \"allenai/specter\"\n",
    "            elif self.retriever_type == 'longformer':\n",
    "                model_name = \"allenai/longformer-base-4096\"\n",
    "            elif self.retriever_type == \"medcpt\":\n",
    "                model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "            else:\n",
    "                model_name = \"facebook/contriever\"\n",
    "\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            text_embedding = self._compute_embeddings(self.texts)\n",
    "            text_embedding = text_embedding.cpu()\n",
    "            self.faiss_index = self._create_dense_retriever(text_embedding)\n",
    "            # print(\"FAISS index created.\")\n",
    "\n",
    "        # Sparse - BM25, TF-IDF\n",
    "        self.bm25_index = self._get_bm25_index(self.texts)\n",
    "\n",
    "    def _compute_embeddings(self, texts):\n",
    "        def mean_pooling(token_embeddings, mask):\n",
    "            token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "            sentence_embeddings = (token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]).detach()\n",
    "            return sentence_embeddings\n",
    "        \n",
    "        all_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 32):\n",
    "                batch_texts = texts[i:i+32]\n",
    "                inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=self.model.config.max_position_embeddings).to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = mean_pooling(outputs[0], inputs['attention_mask']).cpu()\n",
    "                embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "                all_embeddings.append(embeddings)\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    def _compute_query_embedding(self, query):\n",
    "        return self._compute_embeddings([query]).numpy()\n",
    "    \n",
    "    def _create_dense_retriever(self, text_embedding):\n",
    "        d = text_embedding.shape[1]\n",
    "        faiss_index = faiss.IndexFlatIP(d) # [IndexFlatIP, IndexFlatL2]\n",
    "        faiss_index.add(np.ascontiguousarray(text_embedding.numpy()))\n",
    "        return faiss_index\n",
    "    \n",
    "    def _get_dense_results(self, query, k=5):\n",
    "        query_embedding = self._compute_query_embedding(query)\n",
    "        distances, indices = self.faiss_index.search(query_embedding, k)\n",
    "        faiss_results = []\n",
    "        for idx_list, dist_list in zip(indices, distances):\n",
    "            for idx, dist in zip(idx_list, dist_list):\n",
    "                faiss_results.append((self.texts[idx], dist))\n",
    "        return faiss_results[:k]\n",
    "    \n",
    "    def _get_bm25_index(self, texts):\n",
    "        tokenized_corpus = [text.lower().split() for text in texts]\n",
    "        bm25_index = BM25Okapi(tokenized_corpus)\n",
    "        return bm25_index\n",
    "    \n",
    "    def _get_bm25_results(self, query):\n",
    "        query_tokens = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(query_tokens)\n",
    "        bm25_results = [(self.texts[i], bm25_scores[i]) for i in range(len(bm25_scores))]\n",
    "        bm25_results = sorted(bm25_results, key=lambda x: x[1], reverse=True)\n",
    "        return bm25_results\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        texts = []\n",
    "        for root, _, files in os.walk(self.raw_data_path):  # Recursively walk through directories\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read().strip()\n",
    "                        if self.chunk_size is not None:\n",
    "                            texts.extend(self._split_text(text, self.chunk_size))\n",
    "                        else:\n",
    "                            texts.append(text)  # Add the entire text as a single element\n",
    "\n",
    "                elif file_name.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    if \"summary\" in df.columns:\n",
    "                        texts.extend(df[\"summary\"].dropna().tolist())\n",
    "\n",
    "        if self.chunk_size is not None:\n",
    "            concatenated_text = \" \".join(texts)\n",
    "            texts = self._split_text(concatenated_text, self.chunk_size)\n",
    "        # print(f\"Loaded {len(texts)} documents from directory: {self.raw_data_path}\")\n",
    "        return texts\n",
    "\n",
    "    def _ensemble_scores(self, *results_with_weights):\n",
    "        \"\"\"\n",
    "        Args: results_with_weights: Tuples of (results, weight) where results are lists of (text, score).\n",
    "        Returns: List of (text, combined_score) tuples sorted by score in descending order.\n",
    "        \"\"\"\n",
    "        combined_scores = defaultdict(float)\n",
    "\n",
    "        for results, weight in results_with_weights:\n",
    "            scores_dict = {text: score for text, score in results}\n",
    "            values = np.array(list(scores_dict.values()))\n",
    "            if len(values) > 0:\n",
    "                min_val, max_val = values.min(), values.max()\n",
    "                if max_val > min_val:\n",
    "                    scores_dict = {k: (v - min_val) / (max_val - min_val) for k, v in scores_dict.items()}\n",
    "\n",
    "            for text, score in scores_dict.items():\n",
    "                combined_scores[text] += weight * score\n",
    "\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results\n",
    "    \n",
    "    def _split_text(self, texts, chunk_size, chunk_overlap=50):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        return text_splitter.split_text(texts)\n",
    "\n",
    "    def search(self, query:str, k:int=5):\n",
    "        if self.retriever_type in [\"contriever\", \"longformer\", \"specter\", \"medcpt\"]:\n",
    "            return self._get_dense_results(query, k=k)\n",
    "\n",
    "        elif self.retriever_type == \"bm25\":\n",
    "            return self._get_bm25_results(query)[:k]\n",
    "        \n",
    "        elif self.retriever_type == \"rrf2\":\n",
    "            faiss_results = self._get_dense_results(query, k=k)\n",
    "            bm25_results = self._get_bm25_results(query)\n",
    "            return self._ensemble_scores((faiss_results, 0.5), (bm25_results, 0.5))[:k]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid retriever type. Choose from ['contriever', 'specter', 'medcpt', 'bm25', 'rrf2']\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "slm_model = ChatOllama(model=\"llama3.2:1b\", temperature=0)\n",
    "llm_model = ChatOllama(model=\"gemma:7b\", temperature=0) # $ ollama pull gemma:7b to run a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (No RAG): You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\n",
      "sLM Answer (No RAG): c\n",
      "LLM Answer (No RAG): b\n"
     ]
    }
   ],
   "source": [
    "# Run the model with a prompt\n",
    "def run_simple_prompt(model, question, options):\n",
    "    template = '''You are an AI assistant for grad students.\n",
    "    Answer the question and select the most appropriate answer from the given choices.\n",
    "    You must return only the correct answer (a, b, c or d) and nothing else.\n",
    "    \n",
    "    Question: {question}\n",
    "    Choices:\n",
    "    {options}\n",
    "\n",
    "    You must return a single character (a, b, c or d). Do not provide any explanation.\n",
    "    '''\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    simple_chain = (prompt | model | StrOutputParser())\n",
    "    answer_without_rag = simple_chain.invoke({\"question\": question, \"options\": options})\n",
    "    return answer_without_rag\n",
    "\n",
    "question = \"You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\"\n",
    "options = [\"Buffer Analysis\", \"Network Analysis\", \"Raster Reclassification\", \"Thiessen Polygon Generation\"]\n",
    "slm_answer_without_rag = run_simple_prompt(slm_model, question, options)\n",
    "llm_answer_without_rag = run_simple_prompt(llm_model, question, options)\n",
    "\n",
    "print(\"Query (No RAG):\", question)\n",
    "print(\"sLM Answer (No RAG):\", slm_answer_without_rag)\n",
    "print(\"LLM Answer (No RAG):\", llm_answer_without_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model with a prompt and RAG\n",
    "def run_rag_chain(model, retriever, question, options, k=2):\n",
    "    def format_docs(docs):\n",
    "        if hasattr(docs[0], 'page_content'):\n",
    "            return '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            return '\\n\\n'.join([d[0] for d in docs])\n",
    "        # return '\\n\\n'.join([ d.page_content for d in docs])\n",
    "\n",
    "    template = '''\n",
    "    Here are the relevant documents: {context}\n",
    "    \n",
    "    You are an AI assistant for graduate students.\n",
    "    Use the provided context to select the most accurate answer.\n",
    "    Return only one letter (a, b, c, or d) with no additional text or explanation.\n",
    "\n",
    "    Question: {question}\n",
    "    Potential choices: {options}\n",
    "\n",
    "    Output only one character (a, b, c, or d) with no explanation.\n",
    "    '''\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retrieved_docs = retriever.search(query=question, k=k)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    rag_chain = (prompt | model | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"context\": formatted_context, \"question\": question, \"options\": options}).strip()\n",
    "    return answer\n",
    "\n",
    "# question = \"You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\"\n",
    "# options = [\"Buffer Analysis\", \"Network Analysis\", \"Raster Reclassification\", \"Thiessen Polygon Generation\"]\n",
    "# answer = run_rag_chain(model, question, options)\n",
    "\n",
    "# print(\"Query (RAG):\", question)\n",
    "# print(\"Answer (RAG):\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74571b594e5411abc127e5941a1b01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Medicine', 'Social & Preventive Medicine', 'Pathology',\n",
       "       'Pharmacology', 'Physiology', 'Anatomy', 'Microbiology',\n",
       "       'Gynaecology & Obstetrics', 'Pediatrics', 'Biochemistry'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from question_answering import QuestionAnswering\n",
    "\n",
    "qa = QuestionAnswering(\"MedMCQA\")\n",
    "qna_df = qa.get_question_answering_dataframe()\n",
    "qna_df['subject'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SLM_CONTRIEVER :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 924/1153 [01:38<00:24,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_answer: e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1153/1153 [02:03<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_contriever, 2 / 1024, 22.55, 123.37\n",
      "\n",
      "SLM_SPECTER :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1153/1153 [01:58<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_specter, 2 / 1024, 22.81, 118.79\n",
      "\n",
      "SLM_LONGFORMER :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "100%|██████████| 1153/1153 [02:02<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_longformer, 2 / 1024, 23.07, 122.04\n",
      "\n",
      "SLM_MEDCPT :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1153/1153 [01:54<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_medcpt, 2 / 1024, 22.55, 114.07\n",
      "\n",
      "SLM_BM25 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 924/1153 [01:22<00:20, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_answer: e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1153/1153 [01:42<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_bm25, 2 / 1024, 23.07, 102.65\n",
      "\n",
      "SLM_RRF2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 924/1153 [01:40<00:24,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_answer: e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1153/1153 [02:05<00:00,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_rrf2, 2 / 1024, 22.64, 125.54\n",
      "\n",
      "🎯 Accuracy Result:\n",
      "| Retriever      | K / Chunk Size   |   Accuracy (%) |   Processing Time (s) |\n",
      "|:---------------|:-----------------|---------------:|----------------------:|\n",
      "| SLM_CONTRIEVER | 2 / 1024         |          22.55 |                123.37 |\n",
      "| SLM_SPECTER    | 2 / 1024         |          22.81 |                118.79 |\n",
      "| SLM_LONGFORMER | 2 / 1024         |          23.07 |                122.04 |\n",
      "| SLM_MEDCPT     | 2 / 1024         |          22.55 |                114.07 |\n",
      "| SLM_BM25       | 2 / 1024         |          23.07 |                102.65 |\n",
      "| SLM_RRF2       | 2 / 1024         |          22.64 |                125.54 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "correct_answers = 0\n",
    "total_samples = len(qna_df)\n",
    "\n",
    "chunk_size = 1024\n",
    "k = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "#  [\"llm_wo_knowledge\", \"slm_wo_knowledge\", \"slm_contriever\", \"slm_specter\", \"slm_longformer\", \"slm_medcpt\", \"slm_bm25\", \"slm_rrf2\"]:\n",
    "for retriever_type in [\"slm_contriever\", \"slm_specter\", \"slm_longformer\", \"slm_medcpt\", \"slm_bm25\", \"slm_rrf2\"]:\n",
    "    print(f\"\\n{retriever_type.upper()} :\")\n",
    "    \n",
    "    if retriever_type not in [\"llm_wo_knowledge\", \"slm_wo_knowledge\"]:\n",
    "        retriever_name = retriever_type.split(\"_\")[-1]\n",
    "        if retriever_name == \"random\":\n",
    "            retriever = MyRetriever(\n",
    "                raw_data_path =\"data/text/medmcqa\",\n",
    "                retriever_type=\"contriever\",\n",
    "                chunk_size=chunk_size\n",
    "            )\n",
    "        else:\n",
    "            retriever = MyRetriever(\n",
    "                raw_data_path =\"data/text/medmcqa\",\n",
    "                retriever_type=retriever_name,\n",
    "                chunk_size=chunk_size\n",
    "            )\n",
    "\n",
    "    start_time = time.time()\n",
    "    for idx, (index, sample) in enumerate(tqdm(qna_df.iterrows(), total=len(qna_df))):\n",
    "        question = sample[\"question\"].lower()\n",
    "        options = sample[\"options\"].lower()\n",
    "        gt_answer = sample[\"answer\"].lower()\n",
    "\n",
    "        if retriever_type == \"llm_wo_knowledge\":\n",
    "            pred_answer = run_simple_prompt(llm_model, question, options).lower()\n",
    "        elif retriever_type == \"slm_wo_knowledge\":\n",
    "            pred_answer = run_simple_prompt(slm_model, question, options).lower()\n",
    "        else:\n",
    "            pred_answer = run_rag_chain(slm_model, retriever, question, options, k).lower()\n",
    "\n",
    "        if pred_answer not in ['a', 'b', 'c', 'd']:\n",
    "            print(\"pred_answer:\", pred_answer)\n",
    "\n",
    "        correct_answers += int(pred_answer == gt_answer)\n",
    "    end_time = time.time()\n",
    "\n",
    "    acc = correct_answers / total_samples * 100\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    results.append({\n",
    "        \"Retriever\": retriever_type.upper(),\n",
    "        \"K / Chunk Size\": f\"{k} / {chunk_size}\",\n",
    "        \"Accuracy (%)\": f\"{acc:.2f}\",\n",
    "        \"Processing Time (s)\": f\"{processing_time:.2f}\",\n",
    "    })\n",
    "    print(f\"{retriever_type}, {k} / {chunk_size}, {acc:.2f}, {processing_time:.2f}\")\n",
    "    correct_answers = 0\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\n🎯 Accuracy Result:\")\n",
    "print(results_df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
