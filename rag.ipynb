{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Retriever with Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRetriever:\n",
    "    def __init__(\n",
    "            self,\n",
    "            raw_data_path:str=\"\",\n",
    "            retriever_type:str=\"contriever\",\n",
    "            chunk_size:int=256,\n",
    "            device:str=\"cuda\"\n",
    "        ):\n",
    "\n",
    "        self.raw_data_path = raw_data_path\n",
    "        self.retriever_type = retriever_type\n",
    "        if retriever_type == 'domain_detector':\n",
    "            self.chunk_size = None\n",
    "        else:\n",
    "            self.chunk_size = chunk_size\n",
    "        self.device = device\n",
    "\n",
    "        self.texts = self._load_dataset()\n",
    "\n",
    "        self.reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        \n",
    "        # Dense - Contriever, MedCPT, Specter, RRF-2\n",
    "        if self.retriever_type in [\"contriever\", \"specter\", \"longformer\", \"medcpt\", \"rrf2\"]:\n",
    "            if self.retriever_type == 'contriever':\n",
    "                model_name = \"facebook/contriever\"\n",
    "            elif self.retriever_type == 'specter':\n",
    "                model_name = \"allenai/specter\"\n",
    "            elif self.retriever_type == 'longformer':\n",
    "                model_name = \"allenai/longformer-base-4096\"\n",
    "            elif self.retriever_type == \"medcpt\":\n",
    "                model_name = \"ncbi/MedCPT-Article-Encoder\"\n",
    "            else:\n",
    "                model_name = \"facebook/contriever\"\n",
    "\n",
    "            self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            text_embedding = self._compute_embeddings(self.texts)\n",
    "            text_embedding = text_embedding.cpu()\n",
    "            self.faiss_index = self._create_dense_retriever(text_embedding)\n",
    "            # print(\"FAISS index created.\")\n",
    "\n",
    "        # Sparse - BM25, TF-IDF\n",
    "        self.bm25_index = self._get_bm25_index(self.texts)\n",
    "\n",
    "    def _compute_embeddings(self, texts):\n",
    "        def mean_pooling(token_embeddings, mask):\n",
    "            token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "            sentence_embeddings = (token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]).detach()\n",
    "            return sentence_embeddings\n",
    "        \n",
    "        all_embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(texts), 32):\n",
    "                batch_texts = texts[i:i+32]\n",
    "                inputs = self.tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=self.model.config.max_position_embeddings).to(self.device)\n",
    "                outputs = self.model(**inputs)\n",
    "                embeddings = mean_pooling(outputs[0], inputs['attention_mask']).cpu()\n",
    "                embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "                all_embeddings.append(embeddings)\n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    def _compute_query_embedding(self, query):\n",
    "        return self._compute_embeddings([query]).numpy()\n",
    "    \n",
    "    def _create_dense_retriever(self, text_embedding):\n",
    "        d = text_embedding.shape[1]\n",
    "        faiss_index = faiss.IndexFlatIP(d) # [IndexFlatIP, IndexFlatL2]\n",
    "        faiss_index.add(np.ascontiguousarray(text_embedding.numpy()))\n",
    "        return faiss_index\n",
    "    \n",
    "    def _get_dense_results(self, query, k=5):\n",
    "        query_embedding = self._compute_query_embedding(query)\n",
    "        distances, indices = self.faiss_index.search(query_embedding, k)\n",
    "        faiss_results = []\n",
    "        for idx_list, dist_list in zip(indices, distances):\n",
    "            for idx, dist in zip(idx_list, dist_list):\n",
    "                faiss_results.append((self.texts[idx], dist))\n",
    "        return faiss_results[:k]\n",
    "    \n",
    "    def _get_bm25_index(self, texts):\n",
    "        tokenized_corpus = [text.lower().split() for text in texts]\n",
    "        bm25_index = BM25Okapi(tokenized_corpus)\n",
    "        return bm25_index\n",
    "    \n",
    "    def _get_bm25_results(self, query):\n",
    "        query_tokens = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(query_tokens)\n",
    "        bm25_results = [(self.texts[i], bm25_scores[i]) for i in range(len(bm25_scores))]\n",
    "        bm25_results = sorted(bm25_results, key=lambda x: x[1], reverse=True)\n",
    "        return bm25_results\n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        texts = []\n",
    "        for root, _, files in os.walk(self.raw_data_path):  # Recursively walk through directories\n",
    "            for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read().strip()\n",
    "                        if self.chunk_size is not None:\n",
    "                            texts.extend(self._split_text(text, self.chunk_size))\n",
    "                        else:\n",
    "                            texts.append(text)  # Add the entire text as a single element\n",
    "\n",
    "                elif file_name.endswith(\".csv\"):\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    if \"summary\" in df.columns:\n",
    "                        texts.extend(df[\"summary\"].dropna().tolist())\n",
    "\n",
    "        if self.chunk_size is not None:\n",
    "            concatenated_text = \" \".join(texts)\n",
    "            texts = self._split_text(concatenated_text, self.chunk_size)\n",
    "        # print(f\"Loaded {len(texts)} documents from directory: {self.raw_data_path}\")\n",
    "        return texts\n",
    "\n",
    "    def _ensemble_scores(self, *results_with_weights):\n",
    "        \"\"\"\n",
    "        Args: results_with_weights: Tuples of (results, weight) where results are lists of (text, score).\n",
    "        Returns: List of (text, combined_score) tuples sorted by score in descending order.\n",
    "        \"\"\"\n",
    "        combined_scores = defaultdict(float)\n",
    "\n",
    "        for results, weight in results_with_weights:\n",
    "            scores_dict = {text: score for text, score in results}\n",
    "            values = np.array(list(scores_dict.values()))\n",
    "            if len(values) > 0:\n",
    "                min_val, max_val = values.min(), values.max()\n",
    "                if max_val > min_val:\n",
    "                    scores_dict = {k: (v - min_val) / (max_val - min_val) for k, v in scores_dict.items()}\n",
    "\n",
    "            for text, score in scores_dict.items():\n",
    "                combined_scores[text] += weight * score\n",
    "\n",
    "        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_results\n",
    "    \n",
    "    def _split_text(self, texts, chunk_size, chunk_overlap=50):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        return text_splitter.split_text(texts)\n",
    "\n",
    "    def search(self, query:str, k:int=5, rerank:bool=False):\n",
    "        retrieved_docs = None\n",
    "        if self.retriever_type in [\"contriever\", \"longformer\", \"specter\", \"medcpt\"]:\n",
    "            retrieved_docs = self._get_dense_results(query, k=k)\n",
    "\n",
    "        elif self.retriever_type == \"bm25\":\n",
    "            retrieved_docs = self._get_bm25_results(query)[:k]\n",
    "        \n",
    "        elif self.retriever_type == \"rrf2\":\n",
    "            faiss_results = self._get_dense_results(query, k=k)\n",
    "            bm25_results = self._get_bm25_results(query)\n",
    "            retrieved_docs = self._ensemble_scores((faiss_results, 0.5), (bm25_results, 0.5))[:k]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid retriever type. Choose from ['contriever', 'specter', 'medcpt', 'bm25', 'rrf2']\")\n",
    "        if rerank:\n",
    "            retrieved_docs = self.rerank(query, retrieved_docs, self.reranker, top_n=2) # top_n = final value of k\n",
    "        return retrieved_docs\n",
    "    \n",
    "    def rerank(self, question, docs, reranker, top_n=2):\n",
    "        pairs = [(question, doc.page_content if hasattr(doc, 'page_content') else doc[0]) for doc in docs]\n",
    "        scores = reranker.predict(pairs)\n",
    "        scored_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        return [d[0] for d in scored_docs[:top_n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "slm_model = ChatOllama(model=\"llama3.2:1b\", temperature=0)\n",
    "llm_model = ChatOllama(model=\"gemma:7b\", temperature=0) # $ ollama pull gemma:7b to run a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (No RAG): You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\n",
      "sLM Answer (No RAG): The least directly relevant geoprocessing technique for initial site selection would be 3D visualization or spatial analysis using tools like ArcGIS Pro or QGIS.\n",
      "\n",
      "These tools allow for the creation of detailed, interactive maps that can help visualize and analyze data related to population density, road networks, and healthcare facilities. However, they are not typically used as a primary step in the site selection process, which often involves more straightforward calculations and data analysis using techniques like linear programming or regression modeling.\n",
      "\n",
      "Other relevant geoprocessing techniques for site selection might include:\n",
      "\n",
      "* Spatial autocorrelation analysis\n",
      "* Distance-based metrics (e.g., Euclidean distance, Haversine distance)\n",
      "* Network analysis (e.g., road network analysis)\n",
      "\n",
      "These techniques can help identify areas with high population density, proximity to roads and healthcare facilities, and other relevant factors that could impact the success of a new hospital.\n",
      "LLM Answer (No RAG): The least directly relevant geoprocessing technique for this initial site selection process is **Reclassified Slope Analysis**. This technique is primarily used for analyzing terrain characteristics and is not directly related to the factors mentioned for hospital location selection.\n"
     ]
    }
   ],
   "source": [
    "# Run the model with a prompt\n",
    "def run_simple_prompt(model, question, options=None):\n",
    "    template = None\n",
    "    if options is None:\n",
    "        template = '''You are an AI assistant for grad students.\n",
    "        Answer the following question in concise sentences.\n",
    "        Question: {question}\n",
    "        '''\n",
    "    else:\n",
    "        template = '''You are an AI assistant for grad students.\n",
    "        Answer the question and select the most appropriate answer from the given choices.\n",
    "        You must return only the correct answer (a, b, c or d) and nothing else.\n",
    "        \n",
    "        Question: {question}\n",
    "        Choices:\n",
    "        {options}\n",
    "\n",
    "        You must return a single character (a, b, c or d). Do not provide any explanation.\n",
    "        '''\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    simple_chain = (prompt | model | StrOutputParser())\n",
    "    answer_without_rag = simple_chain.invoke({\"question\": question, \"options\": options}, config={\"timeout\": 30}).strip()\n",
    "    return answer_without_rag\n",
    "\n",
    "question = \"You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\"\n",
    "# options = [\"Buffer Analysis\", \"Network Analysis\", \"Raster Reclassification\", \"Thiessen Polygon Generation\"]\n",
    "options = None\n",
    "slm_answer_without_rag = run_simple_prompt(slm_model, question, options)\n",
    "llm_answer_without_rag = run_simple_prompt(llm_model, question, options)\n",
    "\n",
    "print(\"Query (No RAG):\", question)\n",
    "print(\"sLM Answer (No RAG):\", slm_answer_without_rag)\n",
    "print(\"LLM Answer (No RAG):\", llm_answer_without_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dca4647aa2a41ae9690cfa16dff5de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/845 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bc649af4754350a466c4c02b47dcb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445fbfd7ee264016871640217202ff92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912fe05ef05443a38673981af707bd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473ff7e53c944944925d60e2b8a91c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44d8c32b7e54919a0430d81e45205b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/732 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query (RAG): You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\n",
      "Answer (RAG): ArcGIS Network Analysis.\n"
     ]
    }
   ],
   "source": [
    "# Run the model with a prompt and RAG\n",
    "def run_rag_chain(model, retriever, question, options=None, k=2, is_rerank=False):\n",
    "    def format_docs(docs):\n",
    "        if hasattr(docs[0], 'page_content'):\n",
    "            return '\\n\\n'.join([d.page_content for d in docs])\n",
    "        else:\n",
    "            return '\\n\\n'.join([d[0] for d in docs])\n",
    "        # return '\\n\\n'.join([ d.page_content for d in docs])\n",
    "    \n",
    "    if options is None:\n",
    "        template = '''\n",
    "        Here are the relevant documents: {context}\n",
    "        \n",
    "        You are an AI assistant for graduate students.\n",
    "        Use the provided context to answer the question.\n",
    "        Answer the following question in concise sentences.\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Output only the answer. Do not provide any explanation.\n",
    "        '''\n",
    "    else:\n",
    "        template = '''\n",
    "        Here are the relevant documents: {context}\n",
    "        \n",
    "        You are an AI assistant for graduate students.\n",
    "        Use the provided context to select the most accurate answer.\n",
    "        Return only one letter (a, b, c, or d) with no additional text or explanation.\n",
    "\n",
    "        Question: {question}\n",
    "        Potential choices: {options}\n",
    "\n",
    "        Output only one character (a, b, c, or d) with no explanation.\n",
    "        '''\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    retrieved_docs = retriever.search(query=question, k=k, rerank=is_rerank)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    rag_chain = (prompt | model | StrOutputParser())\n",
    "    answer = rag_chain.invoke({\"context\": formatted_context, \"question\": question, \"options\": options},config={\"timeout\": 30}).strip()\n",
    "    return answer\n",
    "\n",
    "question = \"You are tasked with determining the optimal location for a new hospital, considering factors like population density, proximity to major roads, and distance from existing healthcare facilities. Which geoprocessing technique is least directly relevant to this initial site selection process?\"\n",
    "# options = [\"Buffer Analysis\", \"Network Analysis\", \"Raster Reclassification\", \"Thiessen Polygon Generation\"]\n",
    "options = None\n",
    "retriever = MyRetriever(\n",
    "    raw_data_path =\"data/text/random\",\n",
    "    retriever_type='contriever',\n",
    "    chunk_size=512\n",
    ")\n",
    "answer = run_rag_chain(slm_model, retriever, question, options)\n",
    "\n",
    "print(\"Query (RAG):\", question)\n",
    "print(\"Answer (RAG):\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DATASET = \"MedMCQA\", \"MedQuad\", \"MMLU\"\n",
    "TEST_DATASET = \"MMLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dataset import QuestionAnswering\n",
    "\n",
    "qa = QuestionAnswering(TEST_DATASET)\n",
    "qna_df = qa.get_question_answering_dataframe()\n",
    "# qna_df['subject'].unique() # MedMCQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SLM_LONGFORMER :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for sentence mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_longformer, BLEU: 0.0003, ROUGE-L: 0.0048, BERTScore: 0.4930, Time: 597.62\n",
      "\n",
      "SLM_MEDCPT :\n",
      "Running evaluation for sentence mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:55<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_medcpt, BLEU: 0.0004, ROUGE-L: 0.0053, BERTScore: 0.4933, Time: 595.24\n",
      "\n",
      "SLM_BM25 :\n",
      "Running evaluation for sentence mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [08:59<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_bm25, BLEU: 0.0007, ROUGE-L: 0.0070, BERTScore: 0.4935, Time: 539.30\n",
      "\n",
      "SLM_RRF2 :\n",
      "Running evaluation for sentence mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:13<00:00,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slm_rrf2, BLEU: 0.0006, ROUGE-L: 0.0058, BERTScore: 0.4934, Time: 553.39\n",
      "\n",
      "Evaluation Result:\n",
      "| Retriever      | Rerank   | K / Chunk Size   |   BLEU |   ROUGE-L |   BERTScore |   Processing Time (s) |\n",
      "|:---------------|:---------|:-----------------|-------:|----------:|------------:|----------------------:|\n",
      "| SLM_LONGFORMER | True     | 10 / 512         | 0.0003 |    0.0048 |      0.493  |                597.62 |\n",
      "| SLM_MEDCPT     | True     | 10 / 512         | 0.0004 |    0.0053 |      0.4933 |                595.24 |\n",
      "| SLM_BM25       | True     | 10 / 512         | 0.0007 |    0.007  |      0.4935 |                539.3  |\n",
      "| SLM_RRF2       | True     | 10 / 512         | 0.0006 |    0.0058 |      0.4934 |                553.39 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from bert_score import score\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "MAX_BERT_TOKENS = 512\n",
    "\n",
    "def truncate_and_decode(text, tokenizer, max_len=512):\n",
    "    enc = tokenizer(text, truncation=True, max_length=max_len, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    return tokenizer.decode(enc[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "rouge = Rouge()\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "\n",
    "correct_answers = 0\n",
    "total_samples = len(qna_df)\n",
    "\n",
    "chunk_size = 512\n",
    "k = 10\n",
    "is_rerank = True\n",
    "\n",
    "results = []\n",
    "\n",
    "#  [\"llm_wo_knowledge\", \"slm_wo_knowledge\", \"slm_random\", \"slm_contriever\", \"slm_specter\", \"slm_longformer\", \"slm_medcpt\", \"slm_bm25\", \"slm_rrf2\"]\n",
    "for retriever_type in [\"slm_longformer\", \"slm_medcpt\", \"slm_bm25\", \"slm_rrf2\"]:\n",
    "    print(f\"\\n{retriever_type.upper()} :\")\n",
    "    \n",
    "    if retriever_type not in [\"llm_wo_knowledge\", \"slm_wo_knowledge\"]:\n",
    "        retriever_name = retriever_type.split(\"_\")[-1]\n",
    "        retriever_path = \"random\" if retriever_name == \"random\" else TEST_DATASET.lower()\n",
    "        retriever = MyRetriever(\n",
    "            raw_data_path=f\"data/text/{retriever_path}\",\n",
    "            retriever_type=\"contriever\" if retriever_name == \"random\" else retriever_name,\n",
    "            chunk_size=chunk_size\n",
    "        )\n",
    "\n",
    "    if \"options\" in qna_df.columns and qna_df[\"options\"].apply(lambda x: isinstance(x, list) and len(x) > 0).all():\n",
    "        evaluation_mode = \"multiple_choice\"\n",
    "        print(\"Running evaluation for multiple choice mode...\")\n",
    "    else:\n",
    "        evaluation_mode = \"sentence\"\n",
    "        print(\"Running evaluation for sentence mode...\")\n",
    "\n",
    "    scores_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (index, sample) in enumerate(tqdm(qna_df.iterrows(), total=len(qna_df))):\n",
    "        question = sample[\"question\"].lower()\n",
    "        options = list(map(str.lower, sample[\"options\"])) if evaluation_mode == \"multiple_choice\" else None\n",
    "        gt_answer = sample[\"answer\"].lower()\n",
    "\n",
    "        if retriever_type == \"llm_wo_knowledge\":\n",
    "            pred_answer = run_simple_prompt(llm_model, question, options).lower()\n",
    "        elif retriever_type == \"slm_wo_knowledge\":\n",
    "            pred_answer = run_simple_prompt(slm_model, question, options).lower()\n",
    "        else:\n",
    "            pred_answer = run_rag_chain(slm_model, retriever, question, options, k, is_rerank).lower()\n",
    "        \n",
    "        if evaluation_mode == \"multiple_choice\":\n",
    "            if pred_answer not in ['a', 'b', 'c', 'd']:\n",
    "                print(\"Invalid prediction:\", pred_answer)\n",
    "            correct_answers += int(pred_answer == gt_answer)\n",
    "\n",
    "        else:\n",
    "            bleu = sentence_bleu([gt_answer.split()], pred_answer.split(), smoothing_function=smooth_fn)\n",
    "            rouge_l = rouge.get_scores(pred_answer, gt_answer)[0]['rouge-l']['f']\n",
    "            \n",
    "            # Truncate for BERTScore\n",
    "            truncated_pred = truncate_and_decode(pred_answer, bert_tokenizer)\n",
    "            truncated_gt = truncate_and_decode(gt_answer, bert_tokenizer)\n",
    "            try:\n",
    "                _, _, bert_f1 = score([truncated_pred], [truncated_gt], lang='en', model_type='allenai/scibert_scivocab_uncased')\n",
    "                bert_score_value = bert_f1.item()\n",
    "            except RuntimeError:\n",
    "                bert_score_value = 0.0\n",
    "\n",
    "            scores_list.append({\n",
    "                \"BLEU\": bleu,\n",
    "                \"ROUGE-L\": rouge_l,\n",
    "                \"BERTScore\": bert_f1.item()\n",
    "            })\n",
    "\n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    if evaluation_mode == \"multiple_choice\":\n",
    "        acc = correct_answers / total_samples * 100\n",
    "        results.append({\n",
    "            \"Retriever\": retriever_type.upper(),\n",
    "            \"K / Chunk Size\": f\"{k} / {chunk_size}\",\n",
    "            \"Accuracy (%)\": f\"{acc:.2f}\",\n",
    "            \"Processing Time (s)\": f\"{processing_time:.2f}\",\n",
    "        })\n",
    "        print(f\"{retriever_type}, {k} / {chunk_size}, {acc:.2f}, {processing_time:.2f}\")\n",
    "        correct_answers = 0\n",
    "\n",
    "    else:\n",
    "        mean_bleu = sum(s[\"BLEU\"] for s in scores_list) / total_samples\n",
    "        mean_rouge = sum(s[\"ROUGE-L\"] for s in scores_list) / total_samples\n",
    "        mean_bert = sum(s[\"BERTScore\"] for s in scores_list) / total_samples\n",
    "        results.append({\n",
    "            \"Retriever\": retriever_type.upper(),\n",
    "            \"Rerank\": is_rerank,\n",
    "            \"K / Chunk Size\": f\"{k} / {chunk_size}\",\n",
    "            \"BLEU\": f\"{mean_bleu:.4f}\",\n",
    "            \"ROUGE-L\": f\"{mean_rouge:.4f}\",\n",
    "            \"BERTScore\": f\"{mean_bert:.4f}\",\n",
    "            \"Processing Time (s)\": f\"{processing_time:.2f}\",\n",
    "        })\n",
    "        print(f\"{retriever_type}, BLEU: {mean_bleu:.4f}, ROUGE-L: {mean_rouge:.4f}, BERTScore: {mean_bert:.4f}, Time: {processing_time:.2f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nEvaluation Result:\")\n",
    "print(results_df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
